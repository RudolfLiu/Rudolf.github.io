---
layout: post
title: "NLP"
description: "A quick demo of Simple Texture theme's code highlighting features"
categories: [deep-learning]
tags: [nlp]
redirect_from:
  - /2018/12/25/
---

* Kramdown table of contents
{:toc .toc}

# word embedding
***
## ***why use it?*** (vs one-hot)
1. `one-hot`不能表征2个词之间的关系

![alt text](/assets/attached/pic1.PNG "embedding"){:height="50%" width="50%"}

2. 可以起到`降维`的作用
3. 有`类比推理`的特性，使用相似度

![alt text](/assets/attached/pic3.PNG "sim"){:height="50%" width="50%"}

> word embedding常常会伴随着`transfer learning`，背后有庞大的语料库作支撑;
e.x.,将预训练的模型原本10000维的特征embding到100维，即使很小的样本也能很好
的work。

>
![alt text](/assets/attached/pic2.PNG "face encoding"){:height="50%" width="50%"}
>
> word embedding与`face encoding`有这异曲同工之妙，区别在于word embedding有一个固定的语料库。

#### T-SUE
将高维数据能映射到二维上输出显示

***
## ***how to generate word embedding?***
通过使用语料库里的一个词作为上下文，然后使用神经网络来进行预测学习

![alt text](/assets/attached/pic4.PNG "generate"){:height="50%" width="50%"}

### word2Vec Model
* skip-gram

![alt text](/assets/attached/pic5.PNG "gram"){:height="50%" width="50%"}

> 使用softmax会因为维数太高而计算太慢，因此使用改进的`Hierarchical softmax`,
有点类似`快搜`的方法；

`negative sampling`是针对skip-gram高维softmax的另一种解决方案。它将原本的概率问题转化为
二分类问题，将`上下文`的到的word作为target的为正例，而从`词库选`的为负例。词库选择的方式
按照词频3/4选择。

![alt text](/assets/attached/pic6.PNG "sample"){:height="50%" width="50%"}
> k值的选择一般为5~20,对于大语料库则为2~5；语料库越小，k值选择越小。

* CBOW

### GloVe词向量
![alt text](/assets/attached/pic7.PNG "glove"){:height="50%" width="50%"}

f(x)为权值，对于the，is，of这种减小权值。

## 应用
### 情绪分类
### 减少机器学习的人类偏见问题

# Seq2Seq
![alt text](/assets/attached/pic12.PNG "translation"){:height="50%" width="50%"}

## Beam search
`问题`:_生成的众多语句中，如何找到表达最好的_

最大化条件概率模型优化方法没办法使用`贪心法`（搜索空间太高）,而应该使用`beam搜索`

![alt text](/assets/attached/pic13.PNG "translation"){:height="50%" width="50%"}

`问题`:_概率值较小相乘可能造成数值下溢_

上图2式使用log

`问题`:_这样的式子会导致最终的句子长度过短_

上图3式对长度做归一化

## 对RNN模型进行误差分析

![alt text](/assets/attached/pic14.PNG "translation"){:height="50%" width="50%"}

## Bleu打分

![alt text](/assets/attached/pic15.PNG "translation"){:height="50%" width="50%"}

![alt text](/assets/attached/pic16.PNG "translation"){:height="50%" width="50%"}

BP对错误的短句子而得到高分进行惩罚

## Attention Model

`问题`：机器在翻译长句时，往往表现差
